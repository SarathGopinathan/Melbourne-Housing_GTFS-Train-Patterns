{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 3\n",
    "#### Student Name: Sarath Gopinathan\n",
    "#### Student ID: 30434904\n",
    "\n",
    "Date: 13/11/2020\n",
    "\n",
    "Version: 1.4\n",
    "\n",
    "Environment: Python 3.8.5 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* pandas to read and perform actions on the given files\n",
    "* numpy to perform calculations\n",
    "* math to find distance between 2 lat longs\n",
    "* zipfile to unzip the zip files\n",
    "* json to convert json into a list\n",
    "* xml.etree.ElementTree to read the xml\n",
    "* tabula to read pdf\n",
    "* geopandas to get read shp file\n",
    "* shapely.geometry to create Point\n",
    "* datetime to get time\n",
    "* sklearn to perform preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Importing packages](#import_packages)\n",
    "2. [Initializing DataFrame](#init_dataframe)\n",
    "3. [Setting default values](#default_values)\n",
    "4. [Unzipping the files](#unzip)\n",
    "5. [Process of combining all the data](#process)\n",
    "6. [Reading real_state files](#read_files)\n",
    "<br>6.1. [Reading real_state.json](#read_json)\n",
    "<br>6.2. [Reading real_state.xml](#read_xml)\n",
    "<br>6.3. [Merging the two dataframes](#merge)\n",
    "7. [Merging the new dataframe and the initial dataframe](#major_merge)\n",
    "8. [Reading location files(hospital / shopping center / station)](#read_location_files)\n",
    "<br>8.1. [Reading shopingcenters.pdf](#read_pdf)\n",
    "<br>8.2. [Reading supermarkets.xlsx](#read_xlsx)\n",
    "<br>8.3. [Reading hospitals.html](#read_html)\n",
    "<br>8.4. [Reading stops.txt](#read_text)\n",
    "9. [Find closest location(hospital / shopping center / supermarket / station)](#find_closest_location)\n",
    "10. [Find suburb of the location](#find_suburb)\n",
    "11. [Find average time to flinders station from closest stop and transfer flag](#find_avg_time_and_flag)\n",
    "12. [Data reshaping](#data_reshaping)\n",
    "<br>12.1. [Z-Score Normalisation (standardisation)](#z_score)\n",
    "<br>12.2. [MinMax Noramlisation](#min_max)\n",
    "<br>12.3. [Data Transformation](#data_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages <a class=\"anchor\" id=\"import_packages\"></a>\n",
    "\n",
    "Importing all packages required to complete the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tabula-py\n",
    "# pip install geopandas\n",
    "# pip install Shapely==1.2b6\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from tabula import read_pdf\n",
    "import geopandas as gpd\n",
    "import shapefile\n",
    "from shapely.geometry import Point\n",
    "import datetime\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing DataFrame <a class=\"anchor\" id=\"init_dataframe\"></a>\n",
    "\n",
    "Initializing a new Dataframe with all the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Property_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>addr_street</th>\n",
       "      <th>suburb</th>\n",
       "      <th>price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>year</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>Shopping_center_id</th>\n",
       "      <th>Distance_to_sc</th>\n",
       "      <th>Train_station_id</th>\n",
       "      <th>Distance_to_train_station</th>\n",
       "      <th>travel_min_to_CBD</th>\n",
       "      <th>Transfer_flag</th>\n",
       "      <th>Hospital_id</th>\n",
       "      <th>Distance_to_hospital</th>\n",
       "      <th>Supermarket_id</th>\n",
       "      <th>Distance_to_supermaket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Property_id, lat, lng, addr_street, suburb, price, property_type, year, bedrooms, bathrooms, parking_space, Shopping_center_id, Distance_to_sc, Train_station_id, Distance_to_train_station, travel_min_to_CBD, Transfer_flag, Hospital_id, Distance_to_hospital, Supermarket_id, Distance_to_supermaket]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names of the dataframe\n",
    "column_names = ['Property_id', 'lat', 'lng', 'addr_street', 'suburb', 'price', 'property_type', 'year', 'bedrooms', \n",
    "             'bathrooms', 'parking_space', 'Shopping_center_id', 'Distance_to_sc', 'Train_station_id', \n",
    "             'Distance_to_train_station', 'travel_min_to_CBD', 'Transfer_flag', 'Hospital_id', 'Distance_to_hospital', \n",
    "             'Supermarket_id', 'Distance_to_supermaket']\n",
    "\n",
    "# creating dataframe\n",
    "init_df = pd.DataFrame()   \n",
    "\n",
    "init_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "init_df['Distance_to_sc'] = init_df['Distance_to_sc'].astype('float64')\n",
    "init_df['Distance_to_train_station'] = init_df['Distance_to_train_station'].astype('float64')\n",
    "init_df['travel_min_to_CBD'] = init_df['travel_min_to_CBD'].astype('float64')\n",
    "init_df['Distance_to_hospital'] = init_df['Distance_to_hospital'].astype('float64')\n",
    "init_df['Distance_to_supermaket'] = init_df['Distance_to_supermaket'].astype('float64')\n",
    "# head of datafram with only column names\n",
    "init_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting default values <a class=\"anchor\" id=\"default_values\"></a>\n",
    "\n",
    "Function to set all empty values to default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_default_values(df):\n",
    "    \n",
    "    def_df = df\n",
    "    \n",
    "    def_df['suburb'] = \"not available\"\n",
    "    def_df['Shopping_center_id'] = \"not available\"\n",
    "    def_df['Distance_to_sc'] = 0\n",
    "    def_df['Train_station_id'] = 0\n",
    "    def_df['Distance_to_train_station'] = 0\n",
    "    def_df['travel_min_to_CBD'] = 0\n",
    "    def_df['Transfer_flag'] = -1\n",
    "    def_df['Hospital_id'] = \"not available\"\n",
    "    def_df['Distance_to_hospital'] = 0\n",
    "    def_df['Supermarket_id'] = \"not available\"\n",
    "    def_df['Distance_to_supermaket'] = 0\n",
    "    \n",
    "    return def_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzipping the files <a class=\"anchor\" id=\"unzip\"></a>\n",
    "\n",
    "Unzipping all the input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"./30434904.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "    \n",
    "with zipfile.ZipFile(\"./GTFS_Melbourne_Train_Information.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "    \n",
    "with zipfile.ZipFile(\"./vic_suburb_boundary.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of combining all the data <a class=\"anchor\" id=\"process\"></a>\n",
    "\n",
    "1. Read the two realstate files, convert them into dataframes and then merge them and finally merge the new dataframe with a new dataframe with all the columns and set empty column values with default values.\n",
    "\n",
    "2. Calculate the closest location(hospital / shopping center / supermarket / station) and fill in the id and distance columns respectively.\n",
    "\n",
    "3. Find the suburb and fill the column.\n",
    "\n",
    "4. Find travel_min_to_CBD, Transfer_flag column values and fill them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading real_state files <a class=\"anchor\" id=\"read_files\"></a>\n",
    "\n",
    "Reading the input files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading real_state.json <a class=\"anchor\" id=\"read_json\"></a>\n",
    "\n",
    "Initially the json file is read. Then it is loaded as a list using the json.loads() from the json package and then it is converted into a dataframe using pd.json_normalize() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file\n",
    "with open(\"./real_state.json\",'r') as infile:\n",
    "        file = infile.read()\n",
    "        \n",
    "#         file converted to a list\n",
    "        data = json.loads(file)\n",
    "    \n",
    "# list converted to a dataframe\n",
    "real_state_json_df = pd.json_normalize(data)\n",
    "\n",
    "real_state_json_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading real_state.xml <a class=\"anchor\" id=\"read_xml\"></a>\n",
    "\n",
    "Initially the xml file is read. It is identified that there exists a \"b'\" in the start of the file and a \"'\" in the end of the file. This is removed to convert the xml to a dataframe. As observed in the xml, the data fall under the root tag and each of the coulmns are created with the their id tag. Thus we read the xml using ET.XML() method from xml.etree.ElementTree package.\n",
    "\n",
    "Now all the column names are read in a for loop and appended into a list. This is then converted into a dataframe.\n",
    "\n",
    "We observe that the columns are the rows and the rows are the columns and so we take the transpose of this dataframe.\n",
    "\n",
    "The index of the dataframe are the tags of each value and so we reset the index. This makes the existing index into a new column called 'index' and we get a new index with numbers starting from 0 which is what we need.\n",
    "\n",
    "The index column is now deleted as it is of no use.\n",
    "\n",
    "Finally the column names of the columns are renamed to match real_state_json_df inorder to merge them as they have the exact same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading xml file\n",
    "with open(\"./real_state.xml\",'r') as infile:\n",
    "        file = infile.read()\n",
    "# removing \"b'\"\n",
    "file = file[2:]\n",
    "\n",
    "# removing \"'\"\n",
    "file = file[:-1]\n",
    "\n",
    "#reading the xml file\n",
    "root = ET.XML(file)\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# extracting rows and columns as a list\n",
    "for i, child in enumerate(root):\n",
    "        record = {}\n",
    "        for sub_child in child:\n",
    "            record[sub_child.tag] = sub_child.text\n",
    "        all_records.append(record)\n",
    "        \n",
    "\n",
    "# converting the list into dataframe        \n",
    "real_state_xml_df = pd.DataFrame(all_records)\n",
    "\n",
    "# transpose the dataframe  \n",
    "real_state_xml_df = real_state_xml_df.transpose()\n",
    "\n",
    "# new index with numbers\n",
    "real_state_xml_df = real_state_xml_df.reset_index()\n",
    "\n",
    "# removing index column\n",
    "del real_state_xml_df['index']\n",
    "\n",
    "dfcols = ['property_id', 'lat', 'lng', 'addr_street', \n",
    "          'price', 'property_type', 'year', 'bedrooms', 'bathrooms', 'parking_space']\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "# renaming all the column names to match real_state_json_df columns\n",
    "    real_state_xml_df.columns.values[i] = dfcols[i]\n",
    "\n",
    "# change datatype of lan and lng from string to float\n",
    "real_state_xml_df['lat'] = pd.to_numeric(real_state_xml_df['lat'])\n",
    "real_state_xml_df['lng'] = pd.to_numeric(real_state_xml_df['lng'])\n",
    "\n",
    "real_state_xml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two dataframes <a class=\"anchor\" id=\"merge\"></a>\n",
    "\n",
    "real_state_json_df and real_state_xml_df are merged here using concat as both of them have the same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both dataframes to form a single dataframe\n",
    "final_real_state = pd.concat([real_state_json_df, real_state_xml_df])\n",
    "\n",
    "final_real_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the new dataframe and the initial dataframe <a class=\"anchor\" id=\"major_merge\"></a>\n",
    "\n",
    "Merging the init_df and the final_real_state dataframe. Since the property type column names are different, we first peprform outerjoin and then copy values from \"property_id\" to \"Property_id\" and then delete \"property_id\"\n",
    "\n",
    "Next we call the set_default_values method to replace all the empty values with the default values.\n",
    "\n",
    "At the end we get phase_one_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both dataframes using outerjoin\n",
    "phase_one_df = pd.concat([init_df, final_real_state], axis=0, join='outer', ignore_index=False, keys=None,\n",
    "          levels=None, names=None, verify_integrity=False, copy=True)\n",
    "\n",
    "# copy values from column property_id to Property_id\n",
    "phase_one_df['Property_id'] = phase_one_df['property_id']\n",
    "\n",
    "# delete column property_id\n",
    "del phase_one_df['property_id']\n",
    "\n",
    "# new index with numbers\n",
    "phase_one_df = phase_one_df.reset_index()\n",
    "\n",
    "# delete index column\n",
    "del phase_one_df['index']\n",
    "\n",
    "# set default values to empty column values\n",
    "phase_one_df = set_default_values(phase_one_df)\n",
    "\n",
    "phase_one_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find closest location(hospital / shopping center / supermarket / station) <a class=\"anchor\" id=\"read_location_files\"></a>\n",
    "\n",
    "Reading the location files and converting them into respective dataframes. \n",
    "\n",
    "We rename sc_id to id in shopping_center_df to make sure all the dataframes have same column names.\n",
    "We rename and reposition the columns in stops_df to make sure all the four dataframes have same column name and column order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading shopingcenters.pdf <a class=\"anchor\" id=\"read_pdf\"></a>\n",
    "\n",
    "Reading shopingcenters.pdf using the tabula package. In case tabula is not installed, we run the code \"pip install tabula-py\" to install it. Also, jre is required for this to work and so we must have jre in our environment variables after installing in case it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the pdf into a list. Use pages = all to read all the pages\n",
    "shopping_center_list = read_pdf(\"./shopingcenters.pdf\", pages='all')\n",
    "\n",
    "# new dataframe created\n",
    "shopping_center_df = pd.DataFrame();\n",
    "\n",
    "for i in range(0, len(shopping_center_list)):\n",
    "    \n",
    "# append the values in each page using for loop\n",
    "    shopping_center_df = shopping_center_df.append(pd.DataFrame(shopping_center_list[i]))\n",
    "\n",
    "# sorting the dataframe based on the index values in case they are not sorted(chance of paging issue)\n",
    "shopping_center_df.sort_values('Unnamed: 0')\n",
    "\n",
    "# resetting index and making sure it continues from the end of each page instead of starting from 0 again\n",
    "shopping_center_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# deleting the index column\n",
    "del shopping_center_df['Unnamed: 0']\n",
    "\n",
    "# renameing sc_id to id to make sure all the dataframes have same column names\n",
    "shopping_center_df.columns.values[0] = \"id\"\n",
    "\n",
    "shopping_center_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading supermarkets.xlsx <a class=\"anchor\" id=\"read_xlsx\"></a>\n",
    "\n",
    "Reading the supermarkets.xlsx using the read_excel() method in pandas library. We then delete the index column(Unnamed: 0) as it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_df = pd.read_excel('./supermarkets.xlsx')\n",
    "\n",
    "del supermarkets_df['Unnamed: 0']\n",
    "\n",
    "supermarkets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading hospitals.html <a class=\"anchor\" id=\"read_html\"></a>\n",
    "\n",
    "Reading the hospitals.html using the read_html() method in pandas library. A list is created from this and then we convert this into a dataframe. We then delete the index column(Unnamed: 0) as it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "hospitals_list = pd.read_html('./hospitals.html')\n",
    "\n",
    "# create new dataframe\n",
    "hospitals_df = pd.DataFrame();\n",
    "\n",
    "for i in range(0, len(hospitals_list)):\n",
    "    \n",
    "# append the values in the table using for loop\n",
    "    hospitals_df = hospitals_df.append(pd.DataFrame(hospitals_list[i]))\n",
    "\n",
    "# resetting the index with numbers starting from 0\n",
    "hospitals_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# deleting the index column which is not required.\n",
    "del hospitals_df['Unnamed: 0']\n",
    "\n",
    "hospitals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading stops.txt <a class=\"anchor\" id=\"read_text\"></a>\n",
    "\n",
    "Reading the stops.txt using the read_csv() method in pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stops_df = pd.read_csv('./1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/stops.txt', sep=',')\n",
    "\n",
    "# renaming stop_id to id, stop_lat to lat and stop_lon to lng\n",
    "stops_df.columns.values[0] = \"id\"\n",
    "stops_df.columns.values[3] = \"lat\"\n",
    "stops_df.columns.values[4] = \"lng\"\n",
    "\n",
    "# repositioning the columns to make sure all the four dataframes have same column name and column order\n",
    "stops_df = stops_df[['id', 'lat', 'lng', 'stop_name', 'stop_short_name']]\n",
    "\n",
    "stops_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find closest location(hospital / shopping center / supermarket / station) <a class=\"anchor\" id=\"find_closest_location\"></a>\n",
    "\n",
    "Function to calculate and return closest location id and distance based on function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to calculate and find the closest location(hospital / shopping center / station). Returns the closest location \n",
    "# and the distance as a dictionary.\n",
    "\n",
    "def calc_distance(lat1, long1, location_df):\n",
    "    \n",
    "#     empty dictionary created\n",
    "    closest_location = {}\n",
    "    \n",
    "    for i in range (0, location_df['id'].count()):\n",
    "\n",
    "        lat2 = float(location_df.iloc[i][1])\n",
    "        long2 = float(location_df.iloc[i][2])\n",
    "    \n",
    "        # Converts lat & long to spherical coordinates in radians.\n",
    "        degrees_to_radians = math.pi/180.0\n",
    "\n",
    "        # phi = 90 - latitude\n",
    "        phi1 = (90.0 - float(lat1))*degrees_to_radians\n",
    "        phi2 = (90.0 - float(lat2))*degrees_to_radians\n",
    "\n",
    "        # theta = longitude\n",
    "        theta1 = float(long1)*degrees_to_radians\n",
    "        theta2 = float(long2)*degrees_to_radians\n",
    "\n",
    "        cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + math.cos(phi1)*math.cos(phi2))\n",
    "        \n",
    "        #computes distance value using formula\n",
    "        distance = round(math.acos(cos)*6378,3) #radius of the earth in km\n",
    "        \n",
    "#         converts distance from km to m\n",
    "        distance = distance * 1000\n",
    "        \n",
    "#       if it is the first value then write it into the dictionary else check if the existing distance is lesser and then \n",
    "#       write into the dictionary\n",
    "\n",
    "        if(i == 0):\n",
    "            closest_location = {\"id\": location_df.iloc[i][0], \"distance\": distance}\n",
    "        else:\n",
    "            if(closest_location.get(\"distance\") > distance):\n",
    "                closest_location = {\"id\": location_df.iloc[i][0], \"distance\": distance}\n",
    "                \n",
    "    return closest_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(phase_one_df)):\n",
    "    \n",
    "#     find closest shopping center\n",
    "    closest_shopping_center = calc_distance(phase_one_df.iloc[i][1], phase_one_df.iloc[i][2], shopping_center_df)\n",
    "    phase_one_df.at[i,\"Shopping_center_id\"] = closest_shopping_center.get(\"id\")\n",
    "    phase_one_df.at[i,\"Distance_to_sc\"] = closest_shopping_center.get(\"distance\")\n",
    "\n",
    "#     find closest railway stop\n",
    "    closest_stop = calc_distance(phase_one_df.iloc[i][1], phase_one_df.iloc[i][2], stops_df)\n",
    "    phase_one_df.at[i,\"Train_station_id\"] = closest_stop.get(\"id\")\n",
    "    phase_one_df.at[i,\"Distance_to_train_station\"] = closest_stop.get(\"distance\")\n",
    "\n",
    "#     find closest hospital\n",
    "    closest_hospital = calc_distance(phase_one_df.iloc[i][1], phase_one_df.iloc[i][2], hospitals_df)\n",
    "    phase_one_df.at[i,\"Hospital_id\"] = closest_hospital.get(\"id\")\n",
    "    phase_one_df.at[i,\"Distance_to_hospital\"] = closest_hospital.get(\"distance\")\n",
    "\n",
    "#     find closest supermarket\n",
    "    closest_supermarket = calc_distance(phase_one_df.iloc[i][1], phase_one_df.iloc[i][2], supermarkets_df)\n",
    "    phase_one_df.at[i,\"Supermarket_id\"] = closest_supermarket.get(\"id\")\n",
    "    phase_one_df.at[i,\"Distance_to_supermaket\"] = closest_supermarket.get(\"distance\")\n",
    "        \n",
    "phase_two_df = phase_one_df   \n",
    "\n",
    "phase_two_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find suburb of the location <a class=\"anchor\" id=\"find_suburb\"></a>\n",
    "\n",
    "The suburb of the location is identified using package geopandas and shapely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with lat and long values from phase one dataframe\n",
    "latLong_df = pd.DataFrame(columns = ['lat', 'lng'])\n",
    "\n",
    "latLong_df['lat'] = phase_two_df['lat']\n",
    "latLong_df['lng'] = phase_two_df['lng']\n",
    "\n",
    "# create new geopandas dataframe with the latLong_df\n",
    "gdf = gpd.GeoDataFrame(latLong_df, geometry=gpd.points_from_xy(latLong_df.lng, latLong_df.lat))\n",
    "\n",
    "# read the shape file\n",
    "areas = gpd.read_file('./VIC_LOCALITY_POLYGON_shp.shp')\n",
    "\n",
    "# initialise the crs for the new geopandas dataframe\n",
    "gdf.crs = {'init' : areas.crs}\n",
    "\n",
    "# replace the crs with the crs of the areas crs\n",
    "gdf.to_crs(areas.crs, inplace = True)\n",
    "\n",
    "# join both the dataframes using the operation within. This identifies the name of the suburb as both the dataframes merge\n",
    "name = gpd.sjoin(gdf, areas, how = 'inner', op = 'within')\n",
    "\n",
    "# write the suburb value to the main dataframe\n",
    "phase_two_df['suburb'] = name['VIC_LOCA_2']\n",
    "\n",
    "phase_three_df = phase_two_df\n",
    "\n",
    "phase_three_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average time to flinders station from closest stop and transfer flag <a class=\"anchor\" id=\"find_avg_time_and_flag\"></a>\n",
    "\n",
    "Initially we read all the required txt files. \n",
    "Flinders station id is identified and stored.\n",
    "From calendar we notice that only \"T0\" service runs from Monday to Friday. Thus we filter out all the stop times that contain \"T0\" in the trip id.\n",
    "\n",
    "Next extract all the rows that contain stop id of flinders station stop id.\n",
    "Next, we remove all the values which have departure time less than \"07:00:00\" and greater than \"13:00:00\" as that data is useless.\n",
    "Now we merge this dataframe with the stop_times_df as this will give us all the trips which contain flinders station in them.\n",
    "This is used as a base dataframe.\n",
    "\n",
    "Now we pass each station_id from the phase_three_df to find all the trips that start from the respective stop_id between \"07:00:00\" and \"13:00:00\" and find the average time.\n",
    "\n",
    "If the average time is 0 then we change the Transfer_flag to 1 else we keep it as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read trips data\n",
    "trips_df = pd.read_csv('./1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/trips.txt', sep=',')\n",
    "\n",
    "trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stop times data\n",
    "stop_times_df = pd.read_csv('./1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/stop_times.txt', sep=',')\n",
    "\n",
    "stop_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify flinders station stop id from  stops dataframe\n",
    "flinders_df = stops_df[stops_df.isin([\"Flinders Street Railway Station\"]).any(axis=1)]\n",
    "flinders_stop_id = str(flinders_df.iloc[0][0])\n",
    "\n",
    "flinders_stop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read calendar data\n",
    "calendar_df = pd.read_csv('./1. GTFS - Melbourne Train Information - From PTV (9 Oct 2015)/GTFS - Melbourne Train Information/calendar.txt', sep=',')\n",
    "\n",
    "calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only T0 runs on all week days, we filter out rows that contain \"T0\" in the trip_id\n",
    "req_stop_times_df = stop_times_df[stop_times_df['trip_id'].str.contains(\"T0\")]\n",
    "\n",
    "req_stop_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all the rows that have flinders street station id in the stop_id\n",
    "flinders_stop = req_stop_times_df[req_stop_times_df.stop_id == int(flinders_stop_id)]\n",
    "\n",
    "# flinders_stop\n",
    "\n",
    "flinders_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe which will be used to merge late\n",
    "updated_flinders_stop = pd.DataFrame()\n",
    "\n",
    "updated_flinders_stop = pd.DataFrame(columns=flinders_stop.columns)\n",
    "\n",
    "# identify and filter out row if time is less that \"07:00:00\" am or greater than \"13:00:00\"\n",
    "\n",
    "for i in range(0, len(flinders_stop)):\n",
    "    \n",
    "    time = flinders_stop.iloc[i][2]\n",
    "    \n",
    "    hours, minutes, seconds = map(int, time.split(':'))\n",
    "    \n",
    "#     getting seconds from datetime.timedelta\n",
    "    cal_seconds = datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
    "    \n",
    "#     converting to minutes\n",
    "    cal_mins = cal_seconds/60\n",
    "    \n",
    "#     480 = \"07:00:00\" and 780 = \"13:00:00\" - filter the others out\n",
    "    if((cal_mins > 420) and (cal_mins < 780)):\n",
    "        \n",
    "        updated_flinders_stop = updated_flinders_stop.append(flinders_stop.iloc[i].to_frame().transpose())   \n",
    "    \n",
    "# resetting the index with numbers starting from 0\n",
    "updated_flinders_stop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # deleting the index column which is not required.\n",
    "# del updated_flinders_stop['index_col']\n",
    "\n",
    "updated_flinders_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['trip_id','arrival_time','departure_time','stop_id','stop_sequence',\n",
    "        'stop_headsign','pickup_type','drop_off_type','shape_dist_traveled']\n",
    "\n",
    "new_cols = ['trip_id','arrival_time','departure_time','stop_id','stop_sequence',\n",
    "            'stop_headsign','pickup_type','drop_off_type','shape_dist_traveled', 'time_from_flinders']\n",
    "\n",
    "final_time_diff_df = pd.DataFrame(columns=new_cols)\n",
    "\n",
    "# for each item in updated_flinders_stop, we get the entire trip from the stop_times_df by merging both. After merging \n",
    "# a new clolumn is created where the time difference in minutes is stored(time difference between respective stop \n",
    "# and flinders stop). If the time difference is negative, it means it is a return trip and so we ignore those values.\n",
    "for i in range (0, len(updated_flinders_stop)):\n",
    "\n",
    "#     since the iloc.to_frame gives us a transposed dataframe, we take a transpose of it\n",
    "    transposed_df = updated_flinders_stop.iloc[i].to_frame().transpose()\n",
    "\n",
    "#     merge the two dataframes\n",
    "    merged_df = pd.merge(stop_times_df, transposed_df,left_on='trip_id', right_on='trip_id')\n",
    "\n",
    "#     remove all the y values as they are not required\n",
    "    merged_df = merged_df.iloc[:, :-8]\n",
    "\n",
    "#     rename all columns with old column names\n",
    "    merged_df.columns = cols\n",
    "\n",
    "#     find the departure time of flinders station from this dataframe\n",
    "    flinders_stop_time_df = merged_df[merged_df.stop_id == int(flinders_stop_id)]\n",
    "    flinders_stop_time = str(flinders_stop_time_df.iloc[0][2])\n",
    "\n",
    "    flinders_stop_time\n",
    "\n",
    "    hours, minutes, seconds = map(int, flinders_stop_time.split(':'))\n",
    "\n",
    "    flinders_cal_seconds = datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
    "\n",
    "#     calculate the minutes\n",
    "    flinders_cal_mins = flinders_cal_seconds/60\n",
    "\n",
    "#     new list created to store all the time difference values(time difference between station and flinders station)\n",
    "    time_diff = []\n",
    "\n",
    "    for j in range(0, len(merged_df)):\n",
    "\n",
    "        time = merged_df.iloc[j][2]\n",
    "\n",
    "        hours, minutes, seconds = map(int, time.split(':'))\n",
    "\n",
    "        cal_seconds = datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
    "\n",
    "        cal_mins = cal_seconds/60\n",
    "\n",
    "#         append values to list\n",
    "        time_diff.append(flinders_cal_mins - cal_mins)\n",
    "\n",
    "#     add the values to the column\n",
    "    merged_df['time_from_flinders'] = time_diff\n",
    "    \n",
    "#     remove return trips(if time difference is less than 0 it means that the trip is a return trip)\n",
    "    merged_df = merged_df[(merged_df['time_from_flinders'] > 0)]\n",
    "\n",
    "#     append this dataframe into the final data frame which will be used to return avg time based on stop it\n",
    "    final_time_diff_df = final_time_diff_df.append(merged_df)\n",
    "\n",
    "final_time_diff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_time_to_cbd(input_stop_id):\n",
    "    \n",
    "    avg = 0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    total_time = 0\n",
    "    \n",
    "#     subset or required trip\n",
    "    required_stops_df = final_time_diff_df[final_time_diff_df.stop_id == int(input_stop_id)]\n",
    "    \n",
    "    for i in range (0, len(required_stops_df)):\n",
    "        \n",
    "#         calculate time\n",
    "        time = required_stops_df.iloc[i][2]\n",
    "    \n",
    "        hours, minutes, seconds = map(int, time.split(':'))\n",
    "    \n",
    "        cal_seconds = datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
    "    \n",
    "#     calculate minutes\n",
    "        cal_mins = cal_seconds/60\n",
    "    \n",
    "#     check if time is greater than or equal to 7 am and less than or equal to 9 am. \n",
    "# If so add the avg time and add count value by 1\n",
    "        if((cal_mins >= 420) and (cal_mins <= 540)):\n",
    "            \n",
    "            total_time = total_time + required_stops_df.iloc[i][9]\n",
    "            count = count+1\n",
    "            \n",
    "    if(count == 0):\n",
    "        avg = 0\n",
    "    else:\n",
    "        avg = total_time/count \n",
    "    \n",
    "#     return the average value\n",
    "    return avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the average time to flinders street and the transfer flag\n",
    "for i in range(0, len(phase_three_df)):\n",
    "    \n",
    "    avg_time = avg_time_to_cbd(phase_three_df.iloc[i][13])\n",
    "    phase_three_df.at[i,'travel_min_to_CBD'] = avg_time\n",
    "    \n",
    "#     if average time is 0 it means that there are no direct trains or the stop id is flinders \n",
    "#     street stop and so assign flag to 1 else to 0\n",
    "    if(avg_time == 0):\n",
    "        \n",
    "        phase_three_df.at[i,'Transfer_flag'] = 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        phase_three_df.at[i,'Transfer_flag'] = 0\n",
    "    \n",
    "phase_four_df = phase_three_df\n",
    "\n",
    "#writing to new file\n",
    "phase_four_df.to_csv(r'30434904_A3_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reshaping <a class=\"anchor\" id=\"data_reshaping\"></a>\n",
    "\n",
    "The data reshapping process is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Normalisation (standardisation) <a class=\"anchor\" id=\"z_score\"></a>\n",
    "\n",
    "Z-Score normalisation is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(phase_four_df[['price', 'Distance_to_sc', \n",
    "                                                              'travel_min_to_CBD', 'Distance_to_hospital']])\n",
    "df_std = std_scale.transform(phase_four_df[['price', 'Distance_to_sc', 'travel_min_to_CBD', 'Distance_to_hospital']]) # an array not a df\n",
    "df_std[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df['Pscaled'] = df_std[:,0] # so 'Pscaled' is Price scaled\n",
    "phase_four_df['DTSCscaled'] = df_std[:,1] # and 'DTSCscaled' is Distance_to_sc scaled\n",
    "phase_four_df['Tscaled'] = df_std[:,2] # and 'Tscaled' is travel_min_to_CBD scaled\n",
    "phase_four_df['DTHscaled'] = df_std[:,3] # and 'DTHscaled' is Distance_to_hospital scaled\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean after standardisation:\\nprice = {:.2f}, Distance_to_sc = {:.2f}, travel_min_to_CBD = {:.2f}, Distance_to_hospital = {:.2f}'\n",
    "      .format(df_std[:,0].mean(), df_std[:,1].mean(), df_std[:,2].mean(), df_std[:,3].mean()))\n",
    "print('\\nStandard deviation after standardisation:\\nprice = {:.2f}, Distance_to_sc = {:.2f}, travel_min_to_CBD = {:.2f}, Distance_to_hospital = {:.2f}'\n",
    "      .format(df_std[:,0].std(), df_std[:,1].std(), df_std[:,2].std(), df_std[:,3].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"price\"].astype(float).plot(), phase_four_df[\"Distance_to_sc\"].astype(float).plot(), \n",
    "phase_four_df[\"travel_min_to_CBD\"].astype(float).plot(), phase_four_df[\"Distance_to_hospital\"].astype(float).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"price\"].astype(float).hist(), phase_four_df[\"Distance_to_sc\"].astype(float).hist(), \n",
    "phase_four_df[\"travel_min_to_CBD\"].astype(float).hist(), phase_four_df[\"Distance_to_hospital\"].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Pscaled\"].plot(), phase_four_df[\"DTSCscaled\"].plot(), phase_four_df[\"Tscaled\"].plot(), phase_four_df[\"DTHscaled\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Pscaled\"].hist(), phase_four_df[\"DTSCscaled\"].hist(), phase_four_df[\"Tscaled\"].hist(), phase_four_df[\"DTHscaled\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Pscaled\"].plot(), phase_four_df[\"price\"].astype(float).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Pscaled\"].hist(), phase_four_df[\"price\"].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"DTSCscaled\"].plot(), phase_four_df[\"Distance_to_sc\"].astype(float).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"DTSCscaled\"].hist(), phase_four_df[\"Distance_to_sc\"].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Tscaled\"].plot(), phase_four_df[\"travel_min_to_CBD\"].astype(float).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"Tscaled\"].hist(), phase_four_df[\"travel_min_to_CBD\"].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"DTHscaled\"].plot(), phase_four_df[\"Distance_to_hospital\"].astype(float).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_four_df[\"DTHscaled\"].hist(), phase_four_df[\"Distance_to_hospital\"].astype(float).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that the shape is more or less the same for all the above plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Noramlisation <a class=\"anchor\" id=\"min_max\"></a>\n",
    "\n",
    "MinMax normalisation is done below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using scikit-learn <a class=\"anchor\" id=\"mm_using_scikit\"></a>\n",
    "\n",
    "Calculated using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scale = preprocessing.MinMaxScaler().fit(phase_four_df[['price', 'Distance_to_sc', \n",
    "                                                              'travel_min_to_CBD', 'Distance_to_hospital']])\n",
    "df_minmax = minmax_scale.transform(phase_four_df[['price', 'Distance_to_sc', \n",
    "                                                              'travel_min_to_CBD', 'Distance_to_hospital']])\n",
    "df_minmax[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual <a class=\"anchor\" id=\"mm_manual\"></a>\n",
    "\n",
    "Calculated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minP = phase_four_df.price.astype(float).min()\n",
    "maxP = phase_four_df.price.astype(float).max()\n",
    "minP, maxP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDTSC = phase_four_df.Distance_to_sc.astype(float).min()\n",
    "maxDTSC = phase_four_df.Distance_to_sc.astype(float).max()\n",
    "minDTSC, maxDTSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minT = phase_four_df.travel_min_to_CBD.astype(float).min()\n",
    "maxT = phase_four_df.travel_min_to_CBD.astype(float).max()\n",
    "minT, maxT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minDTH = phase_four_df.Distance_to_hospital.astype(float).min()\n",
    "maxDTH = phase_four_df.Distance_to_hospital.astype(float).max()\n",
    "minDTH, maxDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = phase_four_df.price[0]\n",
    "mmp = (p - minP) / (maxP - minP)\n",
    "mmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = phase_four_df[phase_four_df.price == phase_four_df.price.astype(float).max()].price\n",
    "mmp = (p - minP) / (maxP - minP)\n",
    "mmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsc = phase_four_df.Distance_to_sc[0]\n",
    "mmdtsc = (dtsc - minDTSC) / (maxDTSC - minDTSC)\n",
    "mmdtsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsc = phase_four_df[phase_four_df.Distance_to_sc == phase_four_df.Distance_to_sc.astype(float).max()].Distance_to_sc\n",
    "mmdtsc = (dtsc - minDTSC) / (maxDTSC - minDTSC)\n",
    "mmdtsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = phase_four_df.travel_min_to_CBD[0] # the first value, for practice\n",
    "mmt = (t - minT) / (maxT - minT)\n",
    "mmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = phase_four_df[phase_four_df.travel_min_to_CBD == phase_four_df.travel_min_to_CBD.astype(float).max()].travel_min_to_CBD\n",
    "mmt = (t - minT) / (maxT - minT)\n",
    "mmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dth = phase_four_df.Distance_to_hospital[0]\n",
    "mmdth = (dth - minDTH) / (maxDTH - minDTH)\n",
    "mmdth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dth = phase_four_df[phase_four_df.Distance_to_hospital == phase_four_df.Distance_to_hospital.astype(float).max()].Distance_to_hospital\n",
    "mmdth = (dth - minDTH) / (maxDTH - minDTH)\n",
    "mmdth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean after standardisation:\\nprice = {:.2f}, Distance_to_sc = {:.2f}, travel_min_to_CBD = {:.2f}, Distance_to_hospital = {:.2f}'\n",
    "      .format(df_minmax[:,0].min(), df_minmax[:,1].min(), df_minmax[:,2].min(), df_minmax[:,3].min()))\n",
    "print('\\nStandard deviation after standardisation:\\nprice = {:.2f}, Distance_to_sc = {:.2f}, travel_min_to_CBD = {:.2f}, Distance_to_hospital = {:.2f}'\n",
    "      .format(df_minmax[:,0].max(), df_minmax[:,1].max(), df_minmax[:,2].max(), df_minmax[:,3].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the original, standardised and normalised data values <a class=\"anchor\" id=\"plot_org_std_nor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    f = plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(phase_four_df['price'], phase_four_df['Distance_to_sc'],\n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(df_std[:,0], df_std[:,1], color='red',\n",
    "             label='Standardized u=0, s=1', alpha=0.3)\n",
    "    \n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,1],\n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('price and Distance_to_sc content of the realstate dataset')\n",
    "    plt.xlabel('price')\n",
    "    plt.ylabel('Distance_to_sc')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    f = plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(phase_four_df['price'], phase_four_df['travel_min_to_CBD'],\n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(df_std[:,0], df_std[:,2], color='red',\n",
    "             label='Standardized u=0, s=1', alpha=0.3)\n",
    "    \n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,2],\n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('price and travel_min_to_CBD content of the realstate dataset')\n",
    "    plt.xlabel('price')\n",
    "    plt.ylabel('travel_min_to_CBD')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    f = plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(phase_four_df['price'], phase_four_df['Distance_to_hospital'],\n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(df_std[:,0], df_std[:,3], color='red',\n",
    "             label='Standardized u=0, s=1', alpha=0.3)\n",
    "    \n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,3],\n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('price and Distance_to_hospital content of the realstate dataset')\n",
    "    plt.xlabel('price')\n",
    "    plt.ylabel('Distance_to_hospital')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observerved that the shape is not changed in any of the above plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation <a class=\"anchor\" id=\"data_transformation\"></a>\n",
    "\n",
    "The data transformation process is done below(log, power and boxcox)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df['price'], phase_four_df['Distance_to_sc'], phase_four_df['travel_min_to_CBD'], \n",
    "                phase_four_df['Distance_to_hospital'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log transformation <a class=\"anchor\" id=\"log_transformation\"></a>\n",
    "\n",
    "Log transformation is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['lp'] = None\n",
    "\n",
    "phase_four_df[\"price\"] = phase_four_df[\"price\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"price\"])):\n",
    "    \n",
    "    phase_four_df['lp'][i] = math.log(phase_four_df[\"price\"][i])\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['ldtsc'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_sc\"] = phase_four_df[\"Distance_to_sc\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"Distance_to_sc\"])):\n",
    "    \n",
    "    phase_four_df['ldtsc'][i] = math.log(phase_four_df[\"Distance_to_sc\"][i])\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['ldth'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_hospital\"] = phase_four_df[\"Distance_to_hospital\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"Distance_to_hospital\"])):\n",
    "    \n",
    "    phase_four_df['ldth'][i] = math.log(phase_four_df[\"Distance_to_hospital\"][i])\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.lp, phase_four_df.ldtsc) # and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.lp, phase_four_df.ldth) # and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observerved that the shape is more or less the same in the above plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power transformation <a class=\"anchor\" id=\"power_transformation\"></a>\n",
    "\n",
    "Power transformation is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['lp'] = None\n",
    "\n",
    "phase_four_df[\"price\"] = phase_four_df[\"price\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"price\"])):\n",
    "    \n",
    "    phase_four_df['lp'][i] = math.pow(phase_four_df[\"price\"][i],2)\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['ldtsc'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_sc\"] = phase_four_df[\"Distance_to_sc\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"Distance_to_sc\"])):\n",
    "    \n",
    "    phase_four_df['ldtsc'][i] = math.pow(phase_four_df[\"Distance_to_sc\"][i],2)\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phase_four_df['ldth'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_hospital\"] = phase_four_df[\"Distance_to_hospital\"].astype('float')\n",
    "\n",
    "for i in range(0, len(phase_four_df[\"Distance_to_hospital\"])):\n",
    "    \n",
    "    phase_four_df['ldth'][i] = math.pow(phase_four_df[\"Distance_to_hospital\"][i],2)\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.lp, phase_four_df.ldtsc) # and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.lp, phase_four_df.ldth) # and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observerved that the shape is more or less the same in the above plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxcox transformation <a class=\"anchor\" id=\"boxcox_transformation\"></a>\n",
    "\n",
    "Boxcox transformation is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "phase_four_df['bcp'] = None\n",
    "\n",
    "phase_four_df[\"price\"] = phase_four_df[\"price\"].astype('float')\n",
    "\n",
    "boxcox_data = stats.boxcox(phase_four_df[\"price\"])\n",
    "\n",
    "\n",
    "for i in range(0, len(boxcox_data[0])):\n",
    "    \n",
    "    phase_four_df['bcp'][i] = boxcox_data[0][i]\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "phase_four_df['bcdtsc'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_sc\"] = phase_four_df[\"Distance_to_sc\"].astype('float')\n",
    "\n",
    "boxcox_data = stats.boxcox(phase_four_df[\"Distance_to_sc\"])\n",
    "\n",
    "\n",
    "for i in range(0, len(boxcox_data[0])):\n",
    "    \n",
    "    phase_four_df['bcdtsc'][i] = boxcox_data[0][i]\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "phase_four_df['bcdth'] = None\n",
    "\n",
    "phase_four_df[\"Distance_to_hospital\"] = phase_four_df[\"Distance_to_hospital\"].astype('float')\n",
    "\n",
    "boxcox_data = stats.boxcox(phase_four_df[\"Distance_to_hospital\"])\n",
    "\n",
    "\n",
    "for i in range(0, len(boxcox_data[0])):\n",
    "    \n",
    "    phase_four_df['bcdth'][i] = boxcox_data[0][i]\n",
    "\n",
    "phase_four_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.bcp, phase_four_df.bcdtsc) # and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phase_four_df.bcp, phase_four_df.bcdth) # and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observerved that the shape is more or less the same in the above plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. scipy.stats.boxcox â€” SciPy v1.5.4 Reference Guide. (2020). Retrieved 18 November 2020, from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
